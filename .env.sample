# ===================================
# CypherRay ML Service - Environment Variables
# ===================================

# ----------------------------------
# AI Provider API Keys (Required)
# ----------------------------------
# OpenAI API Key (Primary AI provider - REQUIRED)
OPENAI_API_KEY=sk-your-openai-api-key-here

# Anthropic API Key (Fallback AI provider - Optional but recommended)
ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here

# ----------------------------------
# Server Configuration
# ----------------------------------
# Port to run the server on (default: 5000)
PORT=5000

# Environment mode (development, staging, production)
ENVIRONMENT=development

# ----------------------------------
# Monitoring & Logging (Optional)
# ----------------------------------
# Logfire project name for production monitoring
LOGFIRE_PROJECT_NAME=cypher-ray-llm

# Logfire token for sending logs to Logfire cloud
LOGFIRE_TOKEN=

# Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# ----------------------------------
# Performance & Optimization
# ----------------------------------
# Enable response caching (true/false)
ENABLE_CACHING=true

# Cache TTL in hours
CACHE_TTL_HOURS=24

# Maximum parallel analysis tasks (reduce for low-memory instances)
MAX_PARALLEL_TASKS=2

# Maximum cache size in memory (MB)
MAX_CACHE_SIZE_MB=100

# Use file-based cache (recommended for production)
USE_FILE_CACHE=true

# ----------------------------------
# Binary Analysis Configuration (NEW)
# ----------------------------------
# Minimum function cyclomatic complexity to analyze (filters out trivial functions)
# Lower values = more functions analyzed (slower, higher cost)
# Higher values = fewer functions analyzed (faster, lower cost, may miss simple crypto)
# Recommended: 3 (default from TDD)
MIN_FUNCTION_COMPLEXITY=3

# Maximum tokens per batch for function analysis
# Prevents context overflow when analyzing binaries with 100+ functions
# Recommended: 50000 for Claude Haiku, 30000 for budget constraints
MAX_TOKENS_PER_BATCH=50000

# Enable YARA signature scanning (additional crypto detection layer)
# Requires yara-python to be installed
ENABLE_YARA_SCANNING=true

# ----------------------------------
# Model Configuration Overrides (Optional)
# ----------------------------------
# Override default model selection strategy
# USE_CHEAPEST_FIRST=true
# ENABLE_COST_OPTIMIZATION=true

# ----------------------------------
# CORS Configuration (Optional)
# ----------------------------------
# Allowed CORS origins (comma-separated)
# CORS_ORIGINS=http://localhost:3000,http://localhost:5173,https://your-frontend.com

# ----------------------------------
# Rate Limiting (Optional)
# ----------------------------------
# OpenAI requests per minute limit
# OPENAI_RATE_LIMIT_RPM=500

# Anthropic requests per minute limit  
# ANTHROPIC_RATE_LIMIT_RPM=50

# ----------------------------------
# File Upload Limits (Optional)
# ----------------------------------
# Maximum file size for binary uploads in MB (reduced for 512MB instances)
MAX_FILE_SIZE_MB=50

# Temporary file storage path
TEMP_STORAGE_PATH=/tmp/cypherray

# ----------------------------------
# Memory Optimization (for 512MB instances)
# ----------------------------------
# Maximum memory for Angr analysis in MB
ANGR_MAX_MEMORY_MB=200

# Garbage collection threshold (lower = more aggressive)
GC_THRESHOLD=50

# ----------------------------------
# Production Deployment Notes
# ----------------------------------
# For production deployment on Render/Railway/Heroku:
# 1. Set OPENAI_API_KEY (required)
# 2. Set ANTHROPIC_API_KEY (recommended for fallback)
# 3. Set ENVIRONMENT=production
# 4. Set LOGFIRE_TOKEN if using monitoring
# 5. Set CORS_ORIGINS to your frontend domains
# 6. PORT is usually auto-set by the platform
